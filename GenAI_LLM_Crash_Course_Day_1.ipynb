{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3url4dOA67Z_"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Important\n",
        "- Learn Regular Expression\n",
        "- Tokenization"
      ],
      "metadata": {
        "id": "04QTltxwFm9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Cleaning**"
      ],
      "metadata": {
        "id": "bdHPbz1dErmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text_special_characters(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters using regex\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Example usage\n",
        "original_text = \"Hello!!! This text -- contains ðŸ˜Š various ### special characters and junk words @like these!!!\"\n",
        "cleaned_text = clean_text_special_characters(original_text)\n",
        "print(\"Original Text:\", original_text)\n",
        "print(\"Cleaned Text:\", cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuAco0Vp-RpE",
        "outputId": "cd296905-baad-433b-fd28-7e7ee10f15ef"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Hello!!! This text -- contains ðŸ˜Š various ### special characters and junk words @like these!!!\n",
            "Cleaned Text: hello this text contains various special characters and junk words like these\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Good morning ðŸ˜Š! Let's grab a coffee\"\n",
        "text.encode(\"utf-8\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SayZUBizDlyg",
        "outputId": "84c95ab2-9bc7-412e-aa88-abb49ee690c1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b\"Good morning \\xf0\\x9f\\x98\\x8a! Let's grab a coffee\""
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spell Checking"
      ],
      "metadata": {
        "id": "VJOrEo2sERqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Textblob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQuHlUt_EWNz",
        "outputId": "a6973563-6cc4-4160-9a6d-d221d5ad4022"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from Textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->Textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->Textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->Textblob) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->Textblob) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spelling Chekcing\n",
        "from textblob import TextBlob\n",
        "\n",
        "def correct_spelling(text):\n",
        "    # Create a TextBlob object\n",
        "    blob = TextBlob(text)\n",
        "\n",
        "    # Correct the spelling\n",
        "    corrected_text = blob.correct()\n",
        "\n",
        "    return str(corrected_text)\n",
        "\n",
        "# Example usage\n",
        "original_text = \"I havv a spelinng mistak in thiss sentnce.\"\n",
        "corrected_text = correct_spelling(original_text)\n",
        "print(\"Original Text:\", original_text)\n",
        "print(\"Corrected Text:\", corrected_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgpJBUoND1po",
        "outputId": "eb3a44e1-7e2a-4842-a623-94dd23177585"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: I havv a spelinng mistak in thiss sentnce.\n",
            "Corrected Text: I have a spelling mistake in this sentence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing HTML Tags, URL, ABR-Word Phrase"
      ],
      "metadata": {
        "id": "eux8WlnKFJrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from bs4 import BeautifulSoup # For Web Scraping\n",
        "\n",
        "# Dictionary for chat abbreviations\n",
        "chat_abbr_dict = {\n",
        "    \"u\": \"you\",\n",
        "    \"ur\": \"your\",\n",
        "    \"r\": \"are\",\n",
        "    \"gr8\": \"great\",\n",
        "    \"b4\": \"before\",\n",
        "    \"idk\": \"I don't know\",\n",
        "    \"ttyl\": \"talk to you later\",\n",
        "    # Add more abbreviations as needed\n",
        "}\n",
        "\n",
        "def clean_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Expand chat abbreviations\n",
        "    words = text.split()\n",
        "    expanded_words = [chat_abbr_dict[word] if word in chat_abbr_dict else word for word in words]\n",
        "    text = ' '.join(expanded_words)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Example usage\n",
        "original_text = \"Hey, check this out: <a href='http://example.com'>example</a>! It's gr8, ttyl!\"\n",
        "cleaned_text = clean_text(original_text)\n",
        "print(\"Original Text:\", original_text)\n",
        "print(\"Cleaned Text:\", cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg8ajOx2FBCH",
        "outputId": "d927479d-9843-4417-af16-29873e40496d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Hey, check this out: <a href='http://example.com'>example</a>! It's gr8, ttyl!\n",
            "Cleaned Text: hey check this out example its great talk to you later\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text - Feature Engineering**"
      ],
      "metadata": {
        "id": "3WidDw-REvC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization\n",
        "Tokenization is the process of splitting text into smaller units called tokens. Tokens can be words, sentences, or even subwords, depending on the level of tokenization. Tokenization is a fundamental step in text preprocessing for NLP tasks.\n",
        "\n",
        "#### Types\n",
        "- Word Tokenization\n",
        "- Sentence Tokenization\n",
        "- Sub-Word Tokenization (tokenization : 'toke','iza','tion')"
      ],
      "metadata": {
        "id": "Dks-KAJ1GOSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKBMo18VHeFT",
        "outputId": "6268a9d2-20f2-48ea-eedb-fc80cb8d2d1a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Hello How are you\"\n",
        "words = word_tokenize(text)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYpb5GeyEgPY",
        "outputId": "c97fb4ec-b4dc-485c-990e-8ca560cd441e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'How', 'are', 'you']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"Hello How are you. I hope you're doing well.\"\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZUuXdMUHcSg",
        "outputId": "87d218a9-8a1f-4027-ed38-e7f31a651e03"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello How are you.', \"I hope you're doing well.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Regular Expression\n",
        "word = \"tokenization\"\n",
        "subword_tokens = [word[i:i+2] for i in range(0, len(word), 2)]\n",
        "print(f\"\\nExample: \\\"{word}\\\" â†’ {subword_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9bay9PDH7F_",
        "outputId": "f5d3f1f2-fd6a-4fca-f250-33b33e7336c2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example: \"tokenization\" â†’ ['to', 'ke', 'ni', 'za', 'ti', 'on']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Corpus - Full Text Book\n",
        "- Document - A Single Line\n",
        "- Vocabulary - List of Unique Words in my Text"
      ],
      "metadata": {
        "id": "LU0FTy_iIsdK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming and Lemmatization\n",
        "- Running, Run : Run\n",
        "- Running, Changed - Prefix/Suffix : 'ing','ed','ation'\n",
        "\n",
        "## Stemming:\n",
        "- Changed : Chang\n",
        "- Running : run\n",
        "\n",
        "## Lemmatization:\n",
        "- run, running, runs : run\n",
        "- changed, changing : change\n"
      ],
      "metadata": {
        "id": "My_IT6pKJACX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the Porter Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Example words\n",
        "words = [\"running\", \"ran\", \"runs\", \"easily\", \"fairly\",\"Changed\",\"Changing\"]\n",
        "\n",
        "# Apply stemming\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Original Words:\", words)\n",
        "print(\"Stemmed Words:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFdOiM38IbqE",
        "outputId": "d0dd4aa7-a6a8-495b-8a61-89789dc20d6c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['running', 'ran', 'runs', 'easily', 'fairly', 'Changed', 'Changing']\n",
            "Stemmed Words: ['run', 'ran', 'run', 'easili', 'fairli', 'chang', 'chang']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Initialize the WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Example words\n",
        "words = [\"running\", \"ran\", \"runs\", \"easily\", \"fairly\", \"Changed\",\"Changing\"]\n",
        "\n",
        "# Function to get WordNet POS tag\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "# Apply lemmatization\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
        "\n",
        "print(\"Original Words:\", words)\n",
        "print(\"Lemmatized Words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOUjWlq1KZMb",
        "outputId": "8c6e01cf-1961-450c-fd33-773beea44ac1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['running', 'ran', 'runs', 'easily', 'fairly', 'Changed', 'Changing']\n",
            "Lemmatized Words: ['run', 'ran', 'run', 'easily', 'fairly', 'Changed', 'Changing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stemming and Lemmatization\n",
        "- Stemming words sometime has meaning or sometimes not, but lemmatization words always has meaning or valid words.\n",
        "- Stemming is based on removing suffixes from words, so its fast. But Lemmatization involves word analysis and pos tagging so its slow and complex.\n",
        "- Stemming has less accuracy word wise, but Lemmatization has more accuracy.\n",
        "\n",
        "#### Use Cases\n",
        "- Stemming can be used of Search Engine or Search Application Tasks.\n",
        "- Lemmazation can be used for Sentiment Analysis, Text Classification or Text Analytics."
      ],
      "metadata": {
        "id": "-Y7-9e6OMKwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Representation**\n",
        "- One hot Encoding\n",
        "- Bag of Words\n",
        "- Bag of Words (N-Grams)\n",
        "- TF-IDF\n",
        "- Word2Vec\n",
        "\n",
        "\n",
        "### Steps -\n",
        "- Building the Vocabulary\n",
        "- Operations\n",
        "\n",
        "ONE HOT Encoding\n",
        "-------------------\n",
        "Text = \"I love machine learning\"\n",
        "unique_words = [\"I\",\"love\",\"machine\",\"learning\"]\n",
        "Assign_index: {\"I\":1,\"love\":2,\"machine\":3,\"learning\":4}\n",
        "\n",
        "\"I\" - [1,0,0,0]\n",
        "\"love\" - [0,1,0,0]\n",
        "\"machine\" - [0,0,1,0]\n",
        "\"learning\" - [0,0,0,1]\n",
        "\n",
        "Issues - Sparsity, Length of Vectors\n",
        "\n",
        "\n",
        "BAG OF WORDS\n",
        "-------------------\n",
        "Text = \"I love machine learning, and I also love deep learning\"\n",
        "unique_words\n",
        "assign_index\n",
        "\n",
        "Representation:\n",
        "[2, 2, 1, 2, 1, 1, 1]\n",
        "\n",
        "Advantage over OHE - It capture Some Semantic Meaning.\n",
        "Issues - Out of Vocab, Length of Vectors\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pA3Hls-9M2bg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One Hot Encoding\n",
        "# Define the text\n",
        "text = \"I love machine learning\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = text.split()\n",
        "\n",
        "# Create the vocabulary\n",
        "vocabulary = list(set(words))\n",
        "vocab_size = len(vocabulary)\n",
        "\n",
        "# Create a mapping from word to index\n",
        "word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "\n",
        "# One-hot encode the words\n",
        "def one_hot_encode(word, word_to_index, vocab_size):\n",
        "    one_hot_vector = [0] * vocab_size\n",
        "    one_hot_vector[word_to_index[word]] = 1\n",
        "    return one_hot_vector\n",
        "\n",
        "# One-hot encode the entire text\n",
        "one_hot_encoded_text = [one_hot_encode(word, word_to_index, vocab_size) for word in words]\n",
        "\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"One-Hot Encoded Text:\")\n",
        "for word, one_hot_vector in zip(words, one_hot_encoded_text):\n",
        "    print(f\"{word}: {one_hot_vector}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUpTwQwEKgL5",
        "outputId": "57f38762-b8cd-4abd-c808-1b770b22e87a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['love', 'learning', 'I', 'machine']\n",
            "One-Hot Encoded Text:\n",
            "I: [0, 0, 1, 0]\n",
            "love: [1, 0, 0, 0]\n",
            "machine: [0, 0, 0, 1]\n",
            "learning: [0, 1, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Define the text\n",
        "text = \"I love machine learning, and I also love deep learning\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = text.split()\n",
        "\n",
        "# Count word frequencies\n",
        "word_frequencies = Counter(words)\n",
        "\n",
        "# Create the vocabulary\n",
        "vocabulary = list(word_frequencies.keys())\n",
        "vocab_size = len(vocabulary)\n",
        "\n",
        "# Create a mapping from word to index\n",
        "word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "\n",
        "# Create Bag of Words representation\n",
        "bag_of_words = [word_frequencies[word] for word in vocabulary]\n",
        "\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"Bag of Words Representation:\", bag_of_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrz-7sftRn45",
        "outputId": "5b2aea57-509f-406c-c901-c42135f9db19"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['I', 'love', 'machine', 'learning,', 'and', 'also', 'deep', 'learning']\n",
            "Bag of Words Representation: [2, 2, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# BAG OF WORDS (N-Grams)\n"
      ],
      "metadata": {
        "id": "fdDuYLYdScnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Define the corpus\n",
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"I love natural language processing.\",\n",
        "    \"Machine learning is an exciting field.\",\n",
        "    \"The quick fox is quick.\"\n",
        "]\n",
        "\n",
        "# Create a CountVectorizer object with n-grams\n",
        "# Here, we use ngram_range=(1, 2) for unigrams and bigrams\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "\n",
        "# Fit the vectorizer to the corpus and transform the corpus into a BoW representation\n",
        "bow_representation = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Get the feature names (words and n-grams)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the feature names\n",
        "print(\"Vecobulary:\")\n",
        "print(vectorizer.vocabulary_)\n",
        "\n",
        "# Print the BoW representation\n",
        "print(\"\\nBag of Words Representation:\")\n",
        "print(bow_representation.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCyQ1PoyRsNu",
        "outputId": "bc140899-282e-4e75-a343-097ad08eda3a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vecobulary:\n",
            "{'the quick': 18, 'quick brown': 15, 'brown fox': 1, 'fox jumps': 4, 'jumps over': 7, 'over the': 14, 'the lazy': 17, 'lazy dog': 9, 'love natural': 11, 'natural language': 13, 'language processing': 8, 'machine learning': 12, 'learning is': 10, 'is an': 5, 'an exciting': 0, 'exciting field': 2, 'quick fox': 16, 'fox is': 3, 'is quick': 6}\n",
            "\n",
            "Bag of Words Representation:\n",
            "[[0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0]\n",
            " [1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Text 1 - Hello You are good\n",
        "- Text 2 - Hello You are not good"
      ],
      "metadata": {
        "id": "NCP5ZOabTS8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF_IDF\n",
        "- TF - Term Freq\n",
        "- IDF - Inverse Document Freq\n",
        "\n",
        "\n",
        "skip = 2\n",
        "total words = 10\n",
        "TF = 2/10\n",
        "\n",
        "IDF = log(N/Num.of Documents)\n",
        "\n",
        "TD_IDF = TF * IDF\n",
        "\n",
        "\n",
        "Document 1: \"The quick brown fox jumps over the lazy dog.\" , 2/9, 1/9, 1/9\n",
        "Document 2: \"A brown fox is quick fox.\" 2/6, 1/6, 1/6\n",
        "\n",
        "IDF = log(1/2) = log(A/B)"
      ],
      "metadata": {
        "id": "X4jPrX1ETzaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Define the corpus\n",
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"A brown fox is a quick fox.\"\n",
        "]\n",
        "\n",
        "# Create a TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the corpus and transform the corpus into a TF-IDF representation\n",
        "tfidf_representation = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Get the feature names (terms)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the feature names\n",
        "print(\"Feature Names (Terms):\")\n",
        "print(feature_names)\n",
        "\n",
        "# Print the TF-IDF representation\n",
        "print(\"\\nTF-IDF Representation:\")\n",
        "print(tfidf_representation.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_Qd1mUwS047",
        "outputId": "45d29188-341f-4672-f5ef-b6d06ba3e304"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names (Terms):\n",
            "['brown' 'dog' 'fox' 'is' 'jumps' 'lazy' 'over' 'quick' 'the']\n",
            "\n",
            "TF-IDF Representation:\n",
            "[[0.2306165  0.32412345 0.2306165  0.         0.32412345 0.32412345\n",
            "  0.32412345 0.2306165  0.6482469 ]\n",
            " [0.35409974 0.         0.70819948 0.49767483 0.         0.\n",
            "  0.         0.35409974 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec\n",
        "- Solves - Length, Sparsity, High Dimensionality, Semantic meaning\n",
        "- How - Fix Lenght, Dense Array, Embeddings for the words\n",
        "\n",
        "\n",
        "/        Man Women Boy Girl\n",
        "\n",
        "King\n",
        "Prince\n",
        "Queen\n",
        "Princess"
      ],
      "metadata": {
        "id": "CekG8LNMWGf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Example sentences\n",
        "sentences = [\n",
        "    [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"],\n",
        "    [\"the\", \"lazy\", \"dog\", \"sleeps\"],\n",
        "    [\"the\", \"quick\", \"brown\", \"cat\", \"jumps\"]\n",
        "]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Find similarity between two words\n",
        "word1 = \"dog\"\n",
        "word2 = \"cat\"\n",
        "similarity = model.wv.similarity(word1, word2)\n",
        "print(f\"Similarity between '{word1}' and '{word2}': {similarity:.2f}\")\n",
        "\n",
        "# Find similar words to a given word\n",
        "word = \"fox\"\n",
        "similar_words = model.wv.most_similar(word)\n",
        "print(f\"Words similar to '{word}': {similar_words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpuknhUCVQFf",
        "outputId": "f3095afa-939b-4d2e-dca6-f86e16be19fb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between 'dog' and 'cat': -0.11\n",
            "Words similar to 'fox': [('the', 0.21615321934223175), ('over', 0.04468922317028046), ('cat', 0.0019510718993842602), ('lazy', -0.03278514742851257), ('sleeps', -0.09326908737421036), ('dog', -0.09579558670520782), ('quick', -0.10513807833194733), ('jumps', -0.16937021911144257), ('brown', -0.17323409020900726)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Read the text from the file\n",
        "with open(\"001ssb.txt\", \"r\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokens = word_tokenize(text.lower())\n",
        "\n",
        "# Remove punctuation from tokens\n",
        "tokens = [word.translate(str.maketrans('', '', string.punctuation)) for word in tokens if word.isalpha()]\n",
        "\n",
        "# Train a Word2Vec model\n",
        "model = Word2Vec([tokens], vector_size=100, window=5, min_count=1, sg=1)\n",
        "\n",
        "# Find word embeddings\n",
        "word_embeddings = model.wv\n",
        "\n",
        "# Print word embeddings for selected words\n",
        "print(\"Word Embeddings:\")\n",
        "selected_words = [\"dead\", \"songs\", \"royce\", \"night\", \"forest\"]\n",
        "for word in selected_words:\n",
        "    print(word, \":\", word_embeddings[word])\n",
        "\n",
        "# Find similarity between two same words\n",
        "word1 = \"dead\"\n",
        "word2 = \"dead\"\n",
        "similarity = model.wv.similarity(word1, word2)\n",
        "print(f\"\\nSimilarity between '{word1}' and '{word2}': {similarity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxJBaKn4Yhae",
        "outputId": "fcfc2c94-157d-4b08-bb4f-cede9f397a74"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Embeddings:\n",
            "dead : [-1.36613557e-02 -5.22106234e-03 -3.98949236e-02  1.01797052e-01\n",
            " -6.61216453e-02 -2.95142502e-01  3.50258164e-02  4.37840164e-01\n",
            " -1.72982708e-01 -8.76349434e-02  3.96527722e-02 -2.08202541e-01\n",
            " -1.90473929e-01  7.31824711e-02 -1.15222797e-01 -8.30630660e-02\n",
            "  1.61287189e-02 -1.58948332e-01 -7.70116225e-02 -3.50763917e-01\n",
            "  9.63376164e-02  1.85087517e-01  3.75155061e-01 -2.01797560e-01\n",
            "  3.81664708e-02  1.42399848e-01 -2.06275687e-01 -3.29811610e-02\n",
            " -1.43613338e-01 -6.28686696e-02  2.57043809e-01 -3.93779278e-02\n",
            "  2.07556352e-01 -2.49498665e-01  1.23824090e-01 -2.62888856e-02\n",
            " -2.66149770e-02 -2.93261945e-01  4.97550890e-02 -4.12374198e-01\n",
            "  7.35615566e-02 -2.60034263e-01  8.15649256e-02  1.56080619e-01\n",
            "  3.99069190e-02 -1.20949045e-01 -3.47807497e-01  1.98426411e-01\n",
            "  5.59718199e-02  1.61480367e-01  3.71649079e-02 -2.32466012e-02\n",
            "  3.12347598e-02 -1.19775543e-02  5.80865629e-02  1.10150173e-01\n",
            "  1.87940747e-02 -8.21881965e-02  5.22226021e-02  2.34655499e-01\n",
            "  1.66248143e-01  1.20918959e-01  1.70104161e-01  1.15090914e-01\n",
            " -2.06148595e-01  1.98069900e-01  5.82975335e-02  2.72738516e-01\n",
            " -3.98959577e-01  1.95094451e-01 -1.49678141e-01  1.65685847e-01\n",
            "  2.74448782e-01 -1.25530943e-01  4.93049026e-01  1.81937099e-01\n",
            "  1.45382136e-01  3.44023630e-02 -5.08979745e-02  4.23358148e-03\n",
            " -1.30792201e-01  7.56052807e-02 -8.80162567e-02  4.51203257e-01\n",
            " -9.63716581e-02 -4.20265645e-02  1.72203362e-01  2.74125040e-01\n",
            "  4.64968942e-03  1.49222165e-01  2.49906287e-01  1.78734809e-01\n",
            " -3.77726654e-04  9.50967446e-02  2.83407032e-01 -1.65445000e-01\n",
            "  5.23643866e-02 -1.45533234e-01  2.83599384e-02 -1.10422067e-01]\n",
            "songs : [ 0.00243223  0.00422899  0.0017055   0.00657674  0.00242963 -0.03213628\n",
            "  0.00511135  0.05208652 -0.01322509 -0.01529572  0.00302235 -0.01520617\n",
            " -0.01420937  0.01276738 -0.00596883 -0.01018084 -0.00170792 -0.02256385\n",
            " -0.01260135 -0.0445265   0.020763    0.02351304  0.03776622 -0.01510692\n",
            "  0.0091724   0.00737823 -0.01477252 -0.00371366 -0.0087464  -0.00648196\n",
            "  0.02765702  0.00374004  0.02912415 -0.02470621  0.00464621 -0.01038605\n",
            "  0.00206467 -0.03418594  0.00128992 -0.05212221 -0.00120251 -0.03537915\n",
            "  0.01208054  0.01540205  0.00668861 -0.01855234 -0.03540786  0.03221809\n",
            "  0.01230307  0.01133464  0.01245657 -0.00212594  0.00085833 -0.00419842\n",
            "  0.009598    0.00594742 -0.00494686 -0.01630815  0.00195934  0.03056304\n",
            "  0.02507962  0.00367425  0.0157563   0.02222956 -0.02792497  0.02560989\n",
            "  0.00679641  0.0367177  -0.04127633  0.03297759 -0.00982234  0.02546001\n",
            "  0.02517609 -0.02221699  0.06376036  0.02232043  0.02331321 -0.00534482\n",
            " -0.00388079  0.00336357 -0.02145461  0.01343549 -0.02011269  0.0537417\n",
            " -0.0044173  -0.00075639  0.01094969  0.02199681  0.01058679  0.02645313\n",
            "  0.0273784   0.02905161 -0.00703268  0.0106485   0.04160758 -0.00781791\n",
            "  0.00699913 -0.01436078  0.00751764 -0.00666658]\n",
            "royce : [-0.00300504 -0.02168816 -0.04357224  0.10542309 -0.05302735 -0.3159482\n",
            "  0.02217429  0.4582016  -0.18571268 -0.08623534  0.04424526 -0.22278607\n",
            " -0.20564564  0.08008886 -0.1222624  -0.09803352  0.02112973 -0.15060772\n",
            " -0.09264026 -0.3809676   0.12107422  0.18285373  0.38911682 -0.20180704\n",
            "  0.03007873  0.13985933 -0.21260872 -0.04677136 -0.16159227 -0.08089691\n",
            "  0.27385524 -0.05499544  0.21481003 -0.2520526   0.1399187  -0.02302352\n",
            " -0.02592605 -0.3053709   0.04195722 -0.41643396  0.07648961 -0.26470193\n",
            "  0.08999421  0.16559409  0.0503374  -0.11595219 -0.34379935  0.21007763\n",
            "  0.05233905  0.18226607  0.03856152 -0.02214832  0.03714512  0.01006389\n",
            "  0.06585106  0.11212247  0.00481537 -0.08130564  0.05474126  0.24343848\n",
            "  0.17410345  0.12084568  0.17526478  0.12348352 -0.21405198  0.20965488\n",
            "  0.07968685  0.2739343  -0.41754532  0.20360097 -0.13580042  0.17462371\n",
            "  0.28467727 -0.13955449  0.5161372   0.19775788  0.12771161  0.04462343\n",
            " -0.04063991 -0.00287673 -0.14459766  0.06513219 -0.09333424  0.4707654\n",
            " -0.09039816 -0.05422675  0.1817392   0.2966208   0.00670371  0.15923268\n",
            "  0.26007733  0.1996858   0.00541905  0.11416637  0.29553458 -0.17633665\n",
            "  0.06104754 -0.15283556  0.03898726 -0.11071318]\n",
            "night : [-2.47120101e-04 -2.62307022e-02 -2.15745959e-02  9.32051688e-02\n",
            " -4.57110927e-02 -3.13363969e-01  2.81853415e-02  4.27150905e-01\n",
            " -1.74717903e-01 -8.35528001e-02  5.64720295e-02 -2.16189876e-01\n",
            " -1.99891239e-01  8.92477185e-02 -9.65328738e-02 -8.54894966e-02\n",
            "  6.70345314e-03 -1.51930377e-01 -8.09630826e-02 -3.59889656e-01\n",
            "  1.00520141e-01  1.85685948e-01  3.80148619e-01 -1.94731072e-01\n",
            "  3.89401615e-02  1.42791122e-01 -1.86475545e-01 -2.43720636e-02\n",
            " -1.48046985e-01 -6.48030415e-02  2.63739645e-01 -5.04961200e-02\n",
            "  2.13351190e-01 -2.54080862e-01  1.30496219e-01 -8.16707965e-03\n",
            " -1.47271119e-02 -2.83715844e-01  2.60719042e-02 -4.00781363e-01\n",
            "  6.02302626e-02 -2.44360581e-01  8.81424621e-02  1.60818532e-01\n",
            "  5.10107055e-02 -1.12553738e-01 -3.30227822e-01  1.91477701e-01\n",
            "  5.68352528e-02  1.67850778e-01  3.05738468e-02 -3.42328548e-02\n",
            "  1.49661247e-02  3.32670636e-03  5.74454106e-02  1.07742466e-01\n",
            "  2.67893765e-02 -7.72806555e-02  3.35748382e-02  2.27489844e-01\n",
            "  1.43620759e-01  1.17009051e-01  1.61718622e-01  1.21726133e-01\n",
            " -2.02582359e-01  2.12288469e-01  7.29359537e-02  2.82954454e-01\n",
            " -4.04522777e-01  1.95803612e-01 -1.38951421e-01  1.66004539e-01\n",
            "  2.58936584e-01 -1.22323595e-01  4.99257833e-01  1.83980659e-01\n",
            "  1.39541581e-01  2.97020283e-02 -5.31918481e-02 -1.62645744e-03\n",
            " -1.24209337e-01  7.11909980e-02 -9.55379233e-02  4.35261369e-01\n",
            " -9.59299058e-02 -4.36064452e-02  1.67816043e-01  2.85714984e-01\n",
            "  5.50434506e-03  1.46895841e-01  2.31700361e-01  1.89614639e-01\n",
            "  5.79445530e-03  9.75252092e-02  2.80936003e-01 -1.51628047e-01\n",
            "  6.35122135e-02 -1.29325315e-01  2.27793455e-02 -9.67674032e-02]\n",
            "forest : [ 0.00140361 -0.01710384 -0.02105165  0.07663813 -0.05327446 -0.2270167\n",
            "  0.01160377  0.34418648 -0.12380552 -0.07256411  0.03216001 -0.16125363\n",
            " -0.13865659  0.0598326  -0.08824824 -0.0676503   0.01492272 -0.12399647\n",
            " -0.06029123 -0.26991782  0.0823379   0.14572062  0.2806919  -0.14194311\n",
            "  0.0228669   0.10917819 -0.1415764  -0.02032778 -0.11712214 -0.0544311\n",
            "  0.19075766 -0.03524061  0.15244663 -0.18585353  0.0858307  -0.01389992\n",
            " -0.01338525 -0.22563796  0.02618065 -0.3055939   0.04599435 -0.20391102\n",
            "  0.06793626  0.1184487   0.03849493 -0.08011278 -0.2595947   0.15488502\n",
            "  0.04188752  0.1258843   0.01505714 -0.01914241  0.02909163  0.00457889\n",
            "  0.05058535  0.07316751  0.00643501 -0.07242443  0.03055861  0.16335928\n",
            "  0.11117285  0.0891219   0.12001777  0.08390625 -0.15257698  0.15763842\n",
            "  0.04513332  0.19710255 -0.3103459   0.15288757 -0.10271224  0.12720907\n",
            "  0.21008894 -0.10532365  0.3787215   0.13580915  0.09525418  0.03607627\n",
            " -0.03744858 -0.00404764 -0.11091561  0.04470523 -0.07551519  0.3408146\n",
            " -0.07066925 -0.0344412   0.1261926   0.20455123  0.00207513  0.12183022\n",
            "  0.18292023  0.14064503  0.00570172  0.08150251  0.21953355 -0.11108209\n",
            "  0.05173107 -0.10233917  0.01622228 -0.08551607]\n",
            "\n",
            "Similarity between 'dead' and 'dead': 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word1 = \"iron\"\n",
        "word2 = \"sword\"\n",
        "similarity = model.wv.similarity(word1, word2)\n",
        "print(f\"\\nSimilarity between '{word1}' and '{word2}': {similarity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo22KoSqZPkF",
        "outputId": "579fd68d-83e6-4b7e-ba6c-d8abebd33c1d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Similarity between 'iron' and 'sword': 0.9987897276878357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find most similar words to a given word\n",
        "word = \"bran\"\n",
        "most_similar_words = model.wv.most_similar(word, topn=5)\n",
        "print(f\"\\nMost similar words to '{word}':\")\n",
        "for similar_word, score in most_similar_words:\n",
        "    print(f\"{similar_word}: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYDykV5MZhEj",
        "outputId": "f2eed9a0-f7d2-4f98-d749-2addeff45fda"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Most similar words to 'bran':\n",
            "face: 0.9988780617713928\n",
            "greyjoy: 0.9988757371902466\n",
            "father: 0.9988554120063782\n",
            "your: 0.9988180994987488\n",
            "he: 0.9988071322441101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LoEbEluFaD_B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}